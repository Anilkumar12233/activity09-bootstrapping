---
title: "Activity 9 - Bootstrapping"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidymodels)
library(tibble)
library(dplyr)
library(ggplot2)
```

```{r}
# Load the necessary libraries
library(tibble)
library(dplyr)
library(ggplot2)

# Set a random seed value so we can obtain the same "random" results
set.seed(2023)

# Creating a data frame named as sim_dat
sim_dat <- tibble(
  # Generate 20 random numbers from a uniform distribution between -5 and 5 and store them in the column "x1"
  x1 = runif(20, -5, 5),
  # Generating 20 random numbers from a uniform distribution between 0 and 100 and store them in the column "x2"
  x2 = runif(20, 0, 100),
  # Generating 20 random binary values (0 or 1) with a probability of 0.5 for 1 and store them in the column "x3"
  x3 = rbinom(20, 1, 0.5)
)

# Setting coefficients for the true (population-level) model
b0 <- 2
b1 <- 0.25
b2 <- -0.5
b3 <- 1
sigma <- 1.5

# Generating 20 random errors from a normal distribution with mean 0 and standard deviation "sigma"
errors <- rnorm(20, 0, sigma)

# Mutating the "sim_dat" tibble by adding a new column "y" which represents the response variable
# Calculating the response variable "y" using the true model equation and the generated data
sim_dat <- sim_dat %>% 
  mutate(
    y = b0 + b1*x1 + b2*x2 + b3*x3 + errors,
    # Converting the binary values in column "x3" to categorical "Yes" or "No"
    x3 = case_when(
      x3 == 0 ~ "No",
      TRUE ~ "Yes"
    )
  )

# The true model equation
y <- 2 + 0.25 * sim_dat$x1 - 0.5 * sim_dat$x2 + 1 * (sim_dat$x3 == "Yes") + errors

# Visualize the relationship between y and each x
# Scatter plot for y and x1
ggplot(sim_dat, aes(x = x1, y = y)) + 
  geom_point() + 
  labs(title = "Scatter plot: y vs x1")

# Scatter plot for y and x2
ggplot(sim_dat, aes(x = x2, y = y)) + 
  geom_point() + 
  labs(title = "Scatter plot: y vs x2")

# Box plot for y and x3 (categorical)
ggplot(sim_dat, aes(x = x3, y = y)) + 
  geom_boxplot() + 
  labs(title = "Box plot: y vs x3")

# Visualize the relationships between each pair of x variables
# Scatter plot for x1 and x2
ggplot(sim_dat, aes(x = x1, y = x2)) + 
  geom_point() + 
  labs(title = "Scatter plot: x1 vs x2")

# Box plot for x1 and x3 (categorical)
ggplot(sim_dat, aes(x = x1, y = factor(x3))) + 
  geom_boxplot() + 
  labs(title = "Box plot: x1 vs x3")

# Scatter plot for x2 and x3 (categorical)
ggplot(sim_dat, aes(x = x2, y = factor(x3))) + 
  geom_point() + 
  labs(title = "Scatter plot: x2 vs x3")

```
```{r}
#In summary, the visualizations confirm our previous hypotheses based on the true model equation:

#"y vs x1" shows a positive linear relationship.
#"y vs x2" indicates a weak or no linear association.
#"y vs x3" displays distinct distributions for the "Yes" and "No" categories in "x3".
#Regarding the relationships between each pair of "x" variables:

#"x1 vs x2" exhibits a random distribution, suggesting no clear linear relationship.
#"x1 vs x3" shows higher values of "x1" for the "No" category in "x3".
#"x2 vs x3" displays distinct distributions of "x2" for the "Yes" and "No" categories in "x3".
#These visualizations provide valuable insights and confirm the patterns predicted by the true model equation, enhancing our understanding of the data. 
```

```{r}
mlr_fit <- linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("lm") %>% 
  fit(y ~ x1 + x2 + x3, data = sim_dat)

# Also include the confidence intervals for our estimated slope parameters
tidy(mlr_fit, conf.int = TRUE)
```

```{r}
#In evaluating the accuracy of the linear regression results compared to the population-level model, we consider:

#Coefficient Estimates: Compare estimated slopes with true coefficients.
#Standard Errors: Assess precision of estimates.
#p-values: Evaluate statistical significance of predictors.
#Confidence Intervals: Check if true coefficients lie within intervals.
#R-squared: Measure how well model explains variance.
#Residual Analysis: Examine residuals for model fit.
#Assumption Check: Ensure regression assumptions are met.
#By analyzing these factors, we can assess model accuracy and goodness-of-fit. If estimates closely match true values, assumptions are met, and the model explains variance well, we can consider the results accurate. Otherwise, further investigation and refinement may be needed.

```

```{r}
# Set a random seed value so we can obtain the same "random" results
set.seed(631)

# Generate the 2000 bootstrap samples
boot_samps <- sim_dat %>% 
  bootstraps(times = 2000)

boot_samps
```
```{r}
# Create a function that fits a fixed MLR model to one split dataset
fit_mlr_boots <- function(split) {
  lm(y ~ x1 + x2 + x3, data = analysis(split))
}

# Fit the model to each split and store the information
# Also, obtain the tidy model information
boot_models <- boot_samps %>% 
  mutate(
    model = map(splits, fit_mlr_boots),
    coef_info = map(model, tidy)
    )
```


```{r}
boots_coefs <- boot_models %>% 
  unnest(coef_info)

boots_coefs
```
```{r}
boot_int <- int_pctl(boot_models, statistics = coef_info, alpha = 0.05)
boot_int
```
```{r}
ggplot(boots_coefs, aes(x = estimate)) +
  geom_histogram(bins = 30) +
  facet_wrap( ~ term, scales = "free") +
  geom_vline(data = boot_int, aes(xintercept = .lower), col = "blue") +
  geom_vline(data = boot_int, aes(xintercept = .upper), col = "blue")
```
```{r}
```
#we can assess the accuracy of the population-level model results by comparing the estimated coefficients with the true coefficients, checking the standard errors and p-values for significance, ensuring the true coefficients lie within the confidence intervals, and evaluating the R-squared value. If these metrics indicate that the model estimates are close to the true coefficients, statistically significant, and the model explains a high proportion of variance, we can consider the results to be accurate. 

```{r}
